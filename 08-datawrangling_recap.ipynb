{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Journalism: Recap Data wrangling\n",
    "\n",
    "Penny Sheets and Damian Trilling\n",
    "\n",
    "In this recap notebook, we will exercise with data wrangling techniques, using one of the table scraping tools we learned on Wednesday from the group presentations (thanks, Valerie & William!).\n",
    "\n",
    "Also have a look at this Cheat Sheet, as a nice reminder/guide to data wrangling:\n",
    "https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n",
    "\n",
    "\n",
    "**To follow this example, you need to `pip install tabula-py` first!**\n",
    "\n",
    "(**Note:** You may have to do this via your terminal window, or \"anaconda prompt\" on windows machines, not here in Jupyter itself, but for many of you it may work within juptyer.)\n",
    "\n",
    "Also, have a look at this PDF: http://www.ametsoc.net/sotc2017/StateoftheClimate2017_lowres.pdf\n",
    "We will try to get the table from page 113 ( Global tropical cyclone counts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! This is an example of a table that is *really* messed up, but short, so you might just type it over instead. But that's a great way to demonstrate the techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns out that what is called p. 113 is actually page 133 \n",
    "#(b/c the front matter of the book is numbered differently)\n",
    "#so you can see we will use the tabula.read_pdf command, give the html address, and specify which page to scrape.\n",
    "\n",
    "#Note: If you're getting a JDK error, then you didn't install the JDK as indicated in the email yesterday. \n",
    "\n",
    "df = tabula.read_pdf('http://www.ametsoc.net/sotc2017/StateoftheClimate2017_lowres.pdf',  \n",
    "                     multiple_tables=False, pages=133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we display the table, we can see it's very messily scraped compared to what it looks like in the pdf.\n",
    "df\n",
    "#but don't panic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first get only the rows that really contain data\n",
    "# We can see that the first row seems to be the headers of the table.  And after row 8, it's text from the pdf.\n",
    "# Row 8 is the totals, which we don't really want either, because we can calculate those ourselves of course.\n",
    "# So, we select just rows 1 up through 7 (up to 8), using the 'iloc' method we learned in an earlier notebook.\n",
    "datarows = df.iloc[1:8]\n",
    "datarows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUT, let's fix the index so that it starts with 0 (important for some stuff later ('concatenating'))\n",
    "# Remember, python starts things at 0, not at 1. So a range of 7 would give us 0-6 as values:\n",
    "datarows.index=range(7)\n",
    "datarows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So... pop quiz! What is already correct, and what still needs to be fixed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datarows.iloc[:,0]   # show us all rows, column 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that a lot of good data is tucked into the same cell.  It seems to be separated on spaces.\n",
    "# Let's split the cells on their spaces and retain only the last four values\n",
    "datarows.iloc[:,0].map(lambda x: x.split()[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... let's turn this into its own dataframe (instead of just showing it) \n",
    "# [for this, we first make it a list of lists]\n",
    "tmpdf1 = pd.DataFrame(datarows.iloc[:,0].map(lambda x: x.split()[-4:]).to_list())\n",
    "tmpdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's give it better columnnames:\n",
    "tmpdf1.columns = ['Tropical Depressions', 'Tropical Storms', 'HurricanTropicalCyclon', 'Major HurricanTropicalCyclon']\n",
    "tmpdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's concatenate (=glue together) with our data frame from above\n",
    "newdf = pd.concat([datarows,tmpdf1], axis=1)\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix the first names\n",
    "oldcolumnnames = newdf.columns.to_list()\n",
    "oldcolumnnames[0] = 'Basin'\n",
    "oldcolumnnames[1] = 'SS Cat5'\n",
    "oldcolumnnames[2] = 'ACE'\n",
    "newdf.columns = oldcolumnnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix Basin name - same as above, but we now retain everything UNTIl the last 4 elements, \n",
    "# and then join the first elements with a space again\n",
    "newdf['Basin'] = newdf['Basin'].map(lambda x: \" \".join(x.split()[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wanna reorder?\n",
    "cols = newdf.columns.to_list()\n",
    "neworder = [cols[0], cols[3], cols[4], cols[5], cols[6], cols[1], cols[2]]\n",
    "reconstructed_table = newdf[neworder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
